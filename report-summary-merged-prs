#! /usr/bin/env python

from optparse import OptionParser
import subprocess
import re
import json
import pickle
from pickle import Unpickler
from gitmergesgraph import *
#-----------------------------------------------------------------------------------
#---- Parser Options
#-----------------------------------------------------------------------------------
parser = OptionParser(usage="usage: %prog CMSSW_REPO CMSDIST_REPO COMPARISON_PAIRS" 
                            "\n CMSSW_REPO: location of the cmssw repository. This must be a bare clone ( git clone --bare )"
                            "\n CMSDIST_REPO: location of the cmsdist repository. This must be a normal clone"
			    "\n for example: cmssw.git or /afs/cern.ch/cms/git-cmssw-mirror/cmssw.git"
                            "\n START_DATE: the date of the earliest IB to show. It must be in the format"
			    "\n <year>-<day>-<month>-<hour>"
                            "\n For example:"
			    "\n 2014-10-08-1400")

(options, args) = parser.parse_args()

#-----------------------------------------------------------------------------------
#---- Review of arguments
#-----------------------------------------------------------------------------------

if (len(args)<3):
  print 'not enough arguments\n'
  parser.print_help()
  exit()

# Remember that the cmssw repo is a bare clone while cmsdist is a complete clone
CMSSW_REPO = args[ 0 ]
CMSDIST_REPO = args[ 1 ]
START_DATE = args[ 2 ]

#-----------------------------------------------------------------------------------
#---- Fuctions
#-----------------------------------------------------------------------------------

# reads a line of config.map and returns a dictionary with is parameters
def parse_config_map_line ( line ):
  params = {}
  parts = line.split( ';' )
 
  for part in parts:
    if part == '':
      continue
    key = part.split( '=' )[ 0 ]
    value = part.split( '=' )[ 1 ]
    params[ key ] = value
    
  return params

# gets the list of architectures by reading config.map, they are saved in ARCHITECTURES
# gets the releases branches from config.map, they are saved in RELEASES_BRANCHES
# it maps the branches for all the releases this is to take into account the case in which the base branch 
# is different from the release queue
def get_config_map_params():

  f = open( 'config.map' , 'r' )
  for line in f.readlines():
    params = parse_config_map_line ( line.rstrip() ) 
    print params
    
    arch = params[ 'SCRAM_ARCH' ]
    if arch not in ARCHITECTURES:
      ARCHITECTURES.append( arch )

    release_queue = params[ 'RELEASE_QUEUE' ]
    base_branch = params.get( 'RELEASE_BRANCH' ) 
    if base_branch:
      RELEASES_BRANCHES[ release_queue ] = base_branch
    else:
      RELEASES_BRANCHES[ release_queue ] = release_queue

    sp_rel_name = release_queue.split( '_' )[ 3 ]

    if sp_rel_name != 'X' and sp_rel_name not in SPECIAL_RELEASES:
      SPECIAL_RELEASES.append( sp_rel_name )

    if not params.get( 'DISABLED' ) and release_queue not in RELEASE_QUEUES:
      RELEASE_QUEUES.append( release_queue ) 

  SP_REL_REGEX = "|".join(SPECIAL_RELEASES)
  RELEASE_QUEUES.sort()
 
  print 'ARCHS:'
  print ARCHITECTURES
  print 'RELEASES_BRANCHES:'
  print RELEASES_BRANCHES
  print 'special releases'
  print SPECIAL_RELEASES
  print 'I am going to show:'
  print RELEASE_QUEUES



# Tells if the pr comes from a merge commit it reads the first part of the line that
# was obtained from git log --graph
def is_pr_from_merge_commit(graph_part):
  return graph_part.startswith('|')

def get_pr_number(line_parts):
  number_and_name = line_parts[1].replace('"','')
  number = re.sub(' from .*', '', number_and_name)
  number = re.sub('Merge pull request #', '', number)
  return number

def get_pr_author_login(line_parts):
  number_and_name = line_parts[1].replace('"','')
  name = re.sub('^Merge .* from ', '', number_and_name)
  name = name.split('/')[0]
  return name

def get_pr_commit_hash(line_parts):
  hash = line_parts[0].replace('"','')
  return hash

def get_pr_title(line_parts):
  title = line_parts[2].replace('"','').strip()
  return title

# gets the information of the pull request
def get_info_pr( line , graph ):
  pull_request = {}
  line_parts = line.split(',')
  pull_request['hash'] = get_pr_commit_hash(line_parts)
  pull_request['number'] = get_pr_number(line_parts)
  pull_request['author_login'] = get_pr_author_login(line_parts)
  pull_request['title'] = get_pr_title(line_parts)
  pull_request['url'] = 'https://github.com/cms-sw/cmssw/pull/%d' % int(pull_request['number'])
  pull_request['from_merge_commit'] = graph[pull_request['hash']].is_from_merge
  pull_request['is_merge_commit'] = False
  return pull_request

def get_info_merge_commit( line , graph ):
  merge_commit = {}
  line_parts = line.split(',')
  merge_commit['hash'] = get_pr_commit_hash(line_parts)
  merge_commit['number'] = merge_commit['hash'][0:7]
  merge_commit['is_merge_commit'] = True
  merge_commit['from_merge_commit'] = False
  brought_commits = get_prs_brought_by_commit( graph , merge_commit['hash'] )
  merge_commit['brought_prs'] = [ pr.pr_number for pr in brought_commits ]
  
  return merge_commit

#reads a line of the output of git log and returns the tags that it contains
#if there are no tags it returns an empty list
#it applies filters according to the release queue to only get the 
#tags related to the current release queue
def get_tags_from_line( line , release_queue):
  if 'tags->' not in line:
    return []
  tags_str = line.split('tags->')[1]
  if 'SLHC' in release_queue:
    filter = release_queue[:-6]+'[X|0-9]_SLHC.*'
  else:
    filter = release_queue[:-1]+'[X|0-9].*'

  ## if the tags part is equal to ," there are no tags
  if tags_str != ',"':
    tags = tags_str.split(',',1)[1].strip().replace('(','').replace(')','').split(',')
    #remove te word "tag: "
    tags = [t.replace('tag: ','') for t in tags ]
    #I also have to remove the branch name because it otherwise will always appear
    #I also remove tags that have the  string _DEBUG_TEST, they are used to create test IBs
    tags = [ t for t in tags if re.match(filter,t.strip()) and ( t.strip().replace('"','') != release_queue ) and ( 'DEBUG_TEST' not in t ) ]
    return [ t.replace('"','').replace('tag:','').strip() for t in tags ]
  else:
    return []

# Returns all pull request found in a list of comparisons, it returns then in a 
# dictionary in which the key is the pr number
def get_all_prs(comparisons):
  prs = {}
  for comp in comparisons:
    for pr in comp['merged_prs']:
      prs[pr['number']] = pr
  return prs

#-----------------------------------------------------------------------------------
#---- Fuctions -- Analize Git ouputs
#-----------------------------------------------------------------------------------

def determine_build_error(nErrorInfo):
  a = BuildResultsKeys.COMP_ERROR in nErrorInfo.keys()
  b = BuildResultsKeys.LINK_ERROR in nErrorInfo.keys()
  c = BuildResultsKeys.MISC_ERROR in nErrorInfo.keys()
  d = BuildResultsKeys.DWNL_ERROR in nErrorInfo.keys()
  e = BuildResultsKeys.DICT_ERROR in nErrorInfo.keys()
  f = BuildResultsKeys.PYTHON_ERROR in nErrorInfo.keys()
  return a or b or c or d or e or f

def determine_build_warning(nErrorInfo):
  return BuildResultsKeys.COMP_WARNING in nErrorInfo.keys()

def get_results_one_addOn_file(file):
  look_for_err_cmd = 'grep "failed" %s' %file
  result,err,ret_code = get_output_command(look_for_err_cmd)
  if '0 failed' in result:
    return True
  else:
    return False

#
# given a unitTests-summary.log it determines if the test passed or not
# it returns a tuple, the first element is one of the possible values of PossibleUnitTestResults
# The second element is a dictionary which indicates how many tests failed
#
def get_results_one_unitTests_file(file):
  look_for_err_cmd = 'grep -c "ERROR" %s' %file
  result,err,ret_code = get_output_command(look_for_err_cmd)

  result = result.rstrip()

  details = { 'num_fails':result }

  if result != '0':
    return PossibleUnitTestResults.FAILED,details
  else:
    return PossibleUnitTestResults.PASSED,details
#
# given a runall-report-step123-.log file it returns the result of the relvals
# it returns a tuple, the first element indicates if the tests passed or not
# the second element is a dictionary which shows the details of how many relvals pased
# and how meny failed
#
def get_results_one_relval_file(file):
  read_last_line_command = 'tail -n1 %s' %file
  
  out,err,ret_code = get_output_command(read_last_line_command)

  num_passed_sep = out.split(',')[0].replace( ' tests passed' , '' ).strip()
  num_failed_sep = out.split(',')[1].replace( ' failed' , '' ).strip()

  num_passed = sum( [ int(num) for num in num_passed_sep.split(' ') ] )
  num_failed = sum( [ int(num) for num in num_failed_sep.split(' ') ] )

  details = { 'num_passed' : num_passed,
              'num_failed' : num_failed }


  if 'SLHC' in file:
    if '0 0 0 0 failed' in out:
      return True,details
    else:
      return False,details

  if '0 0 0 0 0 failed' in out:
    return True,details
  else:
    return False,details

#
# Given a logAnalysis.pkl file, it determines if the tests passed or not
# it returns a tuple, the first element is one of the values of PossibleBuildResults
# The second element is a dictionary contaning the details of the results. 
# If the tests are all ok this dictionary is empty
#
def get_results_details_one_build_file(file):
  summFile = open(file,'r')
  pklr = Unpickler(summFile)
  [rel, plat, anaTime]   = pklr.load()
  errorKeys   = pklr.load()
  nErrorInfo  = pklr.load()
  summFile.close()

  if determine_build_error(nErrorInfo):
    return PossibleBuildResults.ERROR,nErrorInfo
  elif determine_build_warning(nErrorInfo):
    return PossibleBuildResults.WARNING,nErrorInfo
  else:
    return PossibleBuildResults.PASSED,nErrorInfo

  return True,nErrorInfo


#
# parses the tests results for each file in output. It distinguishes if it is 
# build, unit tests, relvals, or addon tests logs. The the result of the parsing
# is saved in the parameter results.
# type can be 'relvals', 'utests', 'addON', 'builds'
#
# schema of results:
# {
#   "<IBName>": [ result_arch1, result_arch2, ... result_archN ]
# }
# schema of result_arch
# {
#   "arch"    : "<architecture>"
#   "file"    : "<location of the result>"
#   "passed"  : <true or false> ( if not applicable the value is true )
#   "details" : <details for the tests> ( can be empty if not applicable, but not undefined ) 
# }
#
def analyze_tests_results( output , results , arch , type ):
  for line in output.splitlines():
    m = re.search('CMSSW.*[0-9]/', line)
    if not m:
      print 'Ignoring line:\n%s' % line
      continue
 
    rel_name = line[m.start():m.end()-1]
    result_arch = {}
    result_arch['arch'] = arch
    result_arch['file'] = line

    details = {}
    if type == 'relvals':
      passed,details = get_results_one_relval_file(line)
    elif type == 'utests':
      passed,details = get_results_one_unitTests_file(line)
    elif type == 'addOn':
      passed = get_results_one_addOn_file(line)
    elif type == 'builds':
      passed,details = get_results_details_one_build_file(line)
    else:
      print 'not a valid test type %s' %type
      exit(1)

    result_arch['passed'] = passed
    result_arch['details'] = details

    if rel_name not in results.keys():
      results[rel_name] = []
      
    results[rel_name].append(result_arch)

 
# returns a list of tags based on git log output
# It uses the release queue name to filter the tags, this avoids having
# in the result tags from other queues that may come from automatic merges.
# For example, if release_queue is 7_2_X, it will drop tags like CMSSW_7_2_THREADED_X_2014-09-15-0200
def get_tags( git_log_output , release_queue ):
  tags = []
  for line in git_log_output.splitlines():
    tags += get_tags_from_line(line, release_queue)

  if (len(tags) ==0):
    print "ATTENTION:"
    print "looks like %s has not changed between the tags specified!" % release_queue
    command_to_execute = MAGIC_COMMAND_FIND_FIRST_MERGE_WITH_TAG.replace('END_TAG',release_queue)
    out,err,ret_code = get_output_command(command_to_execute)
    print out
    tags = get_tags_from_line(out, release_queue)
    print tags

  return tags

#returns the number of the day of a tag
#if it is not an IB tag, it returns -1
def get_day_number_tag(tag):
  parts = tag.split("-")
  if len(parts) == 1:
    return -1
  else:
    day = parts[2]
    try:
        return int(day)
    except ValueError:
        return -1


# uses some heuristics to tell if the list of tags seems to be too short
def is_tag_list_suspicious(tags):
  if len(tags) < 7:
    return True
  day_first_tag = get_day_number_tag(tags[-1])
  day_second_tag = get_day_number_tag(tags[-2])
  return day_second_tag-day_first_tag > 1
  
# determines if  the error is because one of the tags does not exist
# this can happen when the branch that is being analyzed has been 
# created recently
def is_recent_branch(err):
  return "unknown revision or path not in the working tree" in err

#-----------------------------------------------------------------------------------
#---- Fuctions -- Execute Magic commands
#-----------------------------------------------------------------------------------

# this calls the git log command with the first tag to look for missing
# tags that were not found previously
def look_for_missing_tags(start_tag, release_queue):
  command_to_execute = MAGIC_COMMAND_FIND_FIRST_MERGE_WITH_TAG.replace('END_TAG',start_tag)
  out,err,ret_code = get_output_command(command_to_execute)
  tags = get_tags_from_line(out, release_queue)
  return tags

# Executes the command that is given as parameter, returns a tuple out,err,ret_code
# with the output, error and return code obtained
def get_output_command(command_to_execute):
  print 'Executing:'
  print command_to_execute

  p = subprocess.Popen(command_to_execute, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
  out,err = p.communicate()
  ret_code = p.returncode

  if ret_code != 0:
    print ret_code
    print 'Error:'
    print err

  return out,err,ret_code

# Gets the tags between start_tag and end_tag, the release_queue is used as a filter
# to ignore tags that are from other releases
def execute_magic_command_tags( start_tag , end_tag , release_queue , release_branch ):

  # if it is a special release queue based on a branch with a different name, I use the release_branch as end tag
  if release_queue == release_branch:
    real_end_tag = end_tag
  else:
    sp_rel_name = release_queue.split( '_' )[ 3 ]
    real_end_tag = end_tag.replace( '_'+sp_rel_name , '' )

  tags = []
  command_to_execute = MAGIC_COMMAND_TAGS.replace( 'START_TAG' , start_tag ).replace( 'END_TAG' , real_end_tag )
  out,err,ret_code = get_output_command(command_to_execute)

  # check if the end_tag exists, but the start_tag doesn't
  # this could mean that the release branch has been created recently
  if ret_code != 0:
    if is_recent_branch(err):
      print 'looks like this branch has been created recently'
      command_to_execute = MAGIC_COMMAND_FIND_ALL_TAGS.replace( 'END_TAG' , real_end_tag ).replace( 'RELEASE_QUEUE' , release_branch )
      out,err,ret_code = get_output_command(command_to_execute)

  tags = get_tags(out,release_queue)
  tags.append(start_tag)

  #check if the tags list could be missing tags
  # this means that the release branch has not changed much from the start_tag
  if is_tag_list_suspicious(tags):
    print 'this list could be missing something!'
    print tags
    new_tags = look_for_missing_tags(start_tag, release_branch)
    tags.pop()
    tags += new_tags    

  tags = [t for t in reversed(tags)]

  return tags

def execute_command_compare_tags(start_tag,end_tag , graph ):
  command_to_execute = MAGIC_COMMAND_PRS.replace('START_TAG',start_tag).replace('END_TAG',end_tag)
  output,err,ret_code = get_output_command(command_to_execute)
  print '------'
  comp = {}
  comp['compared_tags'] = '%s-->%s' % (start_tag,end_tag)
  prs = []
  for line in output.splitlines():
    
    if 'Merge remote branch ' in line:
      pr = get_info_merge_commit( line , graph )
      if pr['brought_prs'] == []:
        continue
    elif 'Merge pull ' in line:
      pr = get_info_pr( line , graph )
    else:
      continue
    
    prs.append(pr)
  comp['merged_prs'] = prs
  return comp

def compare_tags( tags , graph ):
  comparisons = []
  for i in range(len(tags)-1):
    comp = execute_command_compare_tags(tags[i],tags[i+1] , graph )
    comparisons.append(comp)
  return comparisons

#
# Executes the command to get the tags 
# schema of all_tags_found:
# {
#   "<IBName>": { 
#                 "<arch_name>" : "<tag_name>" 
#               }
# }
def execute_magic_command_get_cmsdist_tags(): 
  all_tags_found = {}
  for arch in ARCHITECTURES:
    command_to_execute = MAGIC_COMMAND_CMSDIST_TAGS.replace( 'ARCHITECTURE' , arch )
    out,err,ret_code = get_output_command( command_to_execute )

    for line in out.splitlines():
      m = re.search('CMSSW.*[0-9]/', line)
      if not m:
        print 'Ignoring line:\n%s' % line
        continue
  
      rel_name = line[m.start():m.end()-1]

      if not all_tags_found.get( rel_name ):
        all_tags_found[ rel_name ] = {}

      all_tags_found[ rel_name ][ arch ] = line

  print 'ALL TAGS'
  print all_tags_found
  return all_tags_found 


#
# Executes the a command to get the results for the relvals, unit tests, 
# addon tests, and compitlation tests
# It saves the results in the parameter 'results' 
# type can be 'relvals', 'utests', 'addON', 'builds'
#
def execute_magic_command_find_results( results , type ):
  for arch in ARCHITECTURES:
    if type == 'relvals':
      base_command = MAGIC_COMMAD_FIND_RESULTS_RELVALS
    elif type == 'utests':
      base_command = MAGIC_COMMAND_FIND_RESULTS_UNIT_TESTS
    elif type == 'addOn':
      base_command = MAGIC_COMMAND_FIND_RESULTS_ADDON
    elif type == 'builds':
      base_command = MAGIC_COMMAND_FIND_RESULTS_BUILD
    else:
      print 'not a valid test type %s' %type
      exit(1)

    command_to_execute = base_command.replace( 'ARCHITECTURE' , arch )
    out,err,ret_code = get_output_command( command_to_execute )
    analyze_tests_results( out , results , arch , type )


def print_results(results):
  print "Results:"
  print
  print
  for rq in results:
    print
    print rq['release_name']
    print '/////////////////////////'
    for comp in rq['comparisons']:
      print comp['compared_tags']

      print '\t' + 'HLT Tests: ' + comp['hlt_tests']

      print '\t' + 'Static Checks: ' + comp['static_checks']

      cmsdist_tags = comp['cmsdistTags']
      print '\t' + 'cmsdist Tags:'+ str( cmsdist_tags )

      builds_results = [res['arch']+':'+str(res['passed'])+':'+str(res['details']) for res in comp['builds'] ]
      print '\t' + 'Builds:'+ str(builds_results)

      relvals_results = [res['arch']+':'+str(res['passed'])+":"+str(res['details']) for res in comp['relvals'] ]
      print '\t' + 'RelVals:'+ str(relvals_results)

      utests_results = [res['arch']+':'+str(res['passed']) +':'+str(res['details']) for res in comp['utests'] ]
      print '\t' + 'UnitTests:' + str(utests_results)

      addons_results = [res['arch']+':'+str(res['passed']) for res in comp['addons'] ]
      print '\t' + 'AddOns:' + str(addons_results)

      merged_prs = [pr['number'] for pr in comp['merged_prs']]
      print '\t' + 'PRs:' + str(merged_prs)

      from_merge_commit = [pr['number'] for pr in comp['merged_prs'] if pr['from_merge_commit']]
      print '\t' + 'From merge commit' + str(from_merge_commit)


#
# Iterates over the IBs comparisons, if an IB doesn't had a tag for an architecture, the previous tag is 
# assigned. For example, for arch slc6_amd64_gcc481
# 1. CMSSW_7_1_X_2014-10-02-1500 was built using the tag IB/CMSSW_7_1_X_2014-10-02-1500/slc6_amd64_gcc481 
# 2. There is no tag for CMSSW_7_1_X_2014-10-03-0200 in cmsdist
#
# Then, it assumes that the tag used for CMSSW_7_1_X_2014-10-03-0200 was IB/CMSSW_7_1_X_2014-10-02-1500/slc6_amd64_gcc481
#
def fill_missing_cmsdist_tags( results ):

  for rq in results:
    previous_cmsdist_tags = {}
    for comp in rq['comparisons']:
      rel_name = comp['compared_tags'].split('-->')[1]

      for arch in comp['tests_archs']:
        current_ib_tag_arch = comp[ 'cmsdistTags' ].get( arch )
        if current_ib_tag_arch:
          previous_cmsdist_tags[ arch ] = current_ib_tag_arch
        else:
          if previous_cmsdist_tags.get( arch ):
            comp[ 'cmsdistTags' ][ arch ] = previous_cmsdist_tags[ arch ]
          else:
            comp[ 'cmsdistTags' ][ arch ] = 'Not Found'
           


#
# merges the results of the tests with the structure of the IBs tags and the pull requests
# 
def add_tests_to_results( results, unit_tests, relvals_results , addon_results , build_results, cmsdist_tags_results ):
  for rq in results:
    for comp in rq['comparisons']:
      rel_name = comp['compared_tags'].split('-->')[1]
      rvsres = relvals_results.get(rel_name)
      utres = unit_tests.get(rel_name)
      adonres = addon_results.get(rel_name)
      buildsres = build_results.get(rel_name)
      cmsdist_tags = cmsdist_tags_results.get(rel_name)

      comp[ 'relvals' ] = rvsres if rvsres else []
      comp[ 'utests' ] = utres if utres else []
      comp[ 'addons' ] = adonres if adonres else []
      comp[ 'builds' ] = buildsres if buildsres else []
      comp[ 'cmsdistTags' ] = cmsdist_tags if cmsdist_tags else {}
 
      a = [t['arch'] for t in utres] if utres else []
      b = [t['arch'] for t in rvsres] if rvsres else []
      c = [t['arch'] for t in buildsres] if buildsres else []

      not_complete_archs =  [arch for arch in c if arch not in a]
      for nca in not_complete_archs:
        result = {}
        result['arch'] = nca
        result['file'] = str([res['file'] for res in buildsres if res['arch'] == nca])
        result['passed'] = PossibleUnitTestResults.UNKNOWN
        result['details'] = {}
        comp['utests'].append(result)

      comp['tests_archs'] = list(set(a+b+c))

  fill_missing_cmsdist_tags( results )


def find_static_results(comparisons):
  for comp in comparisons:
    rel_name = comp['compared_tags'].split('-->')[1]
    comp['static_checks'] = find_one_static_check(rel_name) 

def find_hlt_tests_results(comparisons):
  for comp in comparisons:
    rel_name = comp['compared_tags'].split('-->')[1]
    comp['hlt_tests'] = find_one_hlt_test(rel_name)

def find_one_static_check(release_name):
  command_to_execute = MAGIC_COMMAND_FIND_STATIC_CHECKS.replace('RELEASE_NAME',release_name)
  command_to_execute = command_to_execute.replace('ARCHITECTURE','slc5_amd64_gcc481')
  out,err,ret_code = get_output_command(command_to_execute)
  if '200 OK' in out:
    return STATIC_CHECKS_URL.replace('RELEASE_NAME',release_name).replace('ARCHITECTURE','slc5_amd64_gcc481').replace('vocms12','cmssdt')
  else:
    #If I don't find it in slc5 I look for it in slc6
    print 'looking for resuls in slc6'
    command_to_execute = command_to_execute.replace('slc5_amd64_gcc481','slc6_amd64_gcc481')
    out,err,ret_code = get_output_command(command_to_execute)
    if '200 OK' in out:
      print 'results found for slc6'
      return STATIC_CHECKS_URL.replace('RELEASE_NAME',release_name).replace('ARCHITECTURE','slc6_amd64_gcc481').replace('vocms12','cmssdt')
    else:
      return ''

def find_one_hlt_test(release_name):
  command = MAGIC_COMMAND_FIND_HLT_TESTS.replace('RELEASE_NAME',release_name)
  out,err,ret_code = get_output_command(command)
  if '200 OK' in out:
    print 'found'
    return HLT_TESTS_URL.replace('RELEASE_NAME',release_name).replace('vocms12','cmssdt').replace('vocms12','cmssdt')
  else:
    return ''

# reads the results and generates a separated json for each release_queue
def generate_separated_json_results(results):

  for rq in results:

    file_name = rq['release_name'] + ".json"
    out_json = open(file_name, "w")
    json.dump(rq,out_json,indent=4)
    out_json.close()

# Identifies and groups the releases accodring to their prefix 
# For example if the release queues are: 
# CMSSW_7_1_X, CMSSW_7_0_X, CMSSW_6_2_X, CMSSW_5_3_X, CMSSW_7_1_THREADED_X
# CMSSW_7_1_BOOSTIO_X, CMSSW_7_1_ROOT6_X, CMSSW_7_1_GEANT10_X, CMSSW_6_2_X_SLHC
# CMSSW_7_1_DEVEL_X, CMSSW_7_1_CLANG_X, CMSSW_7_2_X, CMSSW_7_2_DEVEL_X, CMSSW_7_2_CLANG_X 
# CMSSW_7_2_GEANT10_X
# It will organize them like this:
# CMSSW_5_3_X: CMSSW_5_3_X 
# CMSSW_7_2_X: CMSSW_7_2_X, CMSSW_7_2_DEVEL_X, CMSSW_7_2_CLANG_X, CMSSW_7_2_GEANT10_X
# CMSSW_6_2_X: CMSSW_6_2_X CMSSW_6_2_X_SLHC
# CMSSW_7_0_X: CMSSW_7_0_X 
# CMSSW_7_1_X: CMSSW_7_1_X, CMSSW_7_1_THREADED_X, CMSSW_7_1_BOOSTIO_X, CMSSW_7_1_ROOT6_X',
#              CMSSW_7_1_GEANT10_X, CMSSW_7_1_DEVEL_X, CMSSW_7_1_CLANG_X
#
# It returns a dictionary in which the keys are the release prefixes, and the values are
# the release queues
def identify_release_groups(results):
  
  releases = [rq['release_name'] for rq in results]
  releases.sort(reverse=True)
 
  groups = {}

  for rel in releases:
    prefix  = rel[:10] + 'X'
    group = groups.get(prefix)
    if not group:
      groups[prefix] = []
    groups[prefix].append(rel)

  keys = groups.keys()
  keys.sort()
  groups['all_prefixes'] = keys
  groups['all_release_queues'] = releases
  return groups

#-----------------------------------------------------------------------------------
#---- Start of execution
#-----------------------------------------------------------------------------------

MAGIC_COMMAND_CMSDIST_TAGS = 'pushd %s; git tag -l IB/*/ARCHITECTURE; popd' % CMSDIST_REPO

# I used this type of concatenation because the string has %s inside
MAGIC_COMMAND_PRS  = 'GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%H,%s", "%b", "tags->,%d"\' START_TAG..END_TAG'

MAGIC_COMMAND_FIND_FIRST_MERGE_WITH_TAG = 'GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%s", "%b", "tags->,%d"\' END_TAG | grep "tags->" | head -n1'

MAGIC_COMMAD_FIND_RESULTS_RELVALS  = 'find /afs/cern.ch/cms/sw/ReleaseCandidates/ARCHITECTURE/www -name runall-report-step123-.log'

MAGIC_COMMAND_TAGS = 'GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%s", "%b", "tags->,%d"\' START_TAG..END_TAG | grep -E "Merge [pull|remote]"'

MAGIC_COMMAND_FIND_RESULTS_UNIT_TESTS = 'find /afs/cern.ch/cms/sw/ReleaseCandidates/ARCHITECTURE/www -name unitTests-summary.log'

MAGIC_COMMAND_FIND_RESULTS_ADDON  = 'find /afs/cern.ch/cms/sw/ReleaseCandidates/ARCHITECTURE/www -name addOnTests.log'

MAGIC_COMMAND_FIND_RESULTS_BUILD = 'find /afs/cern.ch/cms/sw/ReleaseCandidates/ARCHITECTURE/www -name logAnalysis.pkl'

STATIC_CHECKS_URL = 'https://vocms12.cern.ch/SDT/jenkins-artifacts/ib-static-analysis/RELEASE_NAME/ARCHITECTURE/llvm-analysis/index.html'

HLT_TESTS_URL = 'https://vocms12.cern.ch/SDT/jenkins-artifacts/HLT-Validation/RELEASE_NAME/' 

MAGIC_COMMAND_FIND_STATIC_CHECKS = 'curl -s -k --head %s | head -n 1' % STATIC_CHECKS_URL

MAGIC_COMMAND_FIND_HLT_TESTS = 'curl -s -k --head %s | head -n 1' % HLT_TESTS_URL

MAGIC_COMMAND_GET_CONGIG_MAP = 'wget --no-check-certificate https://raw.githubusercontent.com/cms-sw/cms-bot/HEAD/config.map' 

# this will be filled using config.map by get_config_map_params()
ARCHITECTURES = []

# this will be filled using config.map by get_config_map_params()
RELEASES_BRANCHES = {}

# this will be filled using config.map by get_config_map_params() SLHC releases have a different format, so it is hardcoded
SPECIAL_RELEASES = [ 'SLHC' ]

# this will be filled using config.map by get_config_map_params()
SP_REL_REGEX = ""

# These are the release queues that need to be shown, this this will be filled using config.map by get_config_map_params()
RELEASE_QUEUES = []


MAGIC_COMMAND_FIND_ALL_TAGS ='GIT_DIR='+CMSSW_REPO+' git log --merges  --pretty=\'"%s", "%b", "tags->,%d"\' END_TAG | grep -E "Merge [pull|remote]" | grep -E "RELEASE_QUEUE"'

class BuildResultsKeys:
  DICT_ERROR = 'dictError'
  COMP_ERROR = 'compError'
  LINK_ERROR = 'linkError'
  COMP_WARNING = 'compWarning'
  DWNL_ERROR = 'dwnlError'
  MISC_ERROR = 'miscError'
  IGNORE_WARNING = 'ignoreWarning'
  PYTHON_ERROR = 'pythonError'

class PossibleBuildResults:
  PASSED = 'passed'
  WARNING = 'warning'
  ERROR = 'error'

class PossibleUnitTestResults:
  PASSED = 'passed'
  FAILED = 'failed'
  UNKNOWN = 'unknown'

results = []

get_output_command( MAGIC_COMMAND_GET_CONGIG_MAP )

get_config_map_params()

SP_REL_REGEX = "|".join(SPECIAL_RELEASES)

REQUESTED_COMPARISONS = [ ( '%s_%s..%s' % ( rq , START_DATE , rq ) ) for rq in RELEASE_QUEUES ]

for comp in REQUESTED_COMPARISONS:
  start_tag = comp.split("..")[0]
  end_tag = comp.split("..")[1]
  release_queue = start_tag
  # if is a SLHC or any special release, the split will happen with the fifth underscore _
  if re.search(SP_REL_REGEX,release_queue):
    print 'This is a special release'
    release_queue = re.match(r'^((?:[^_]*_){%d}[^_]*)_(.*)' % (4), release_queue).groups()[0]
  else:
    release_queue = re.match(r'^((?:[^_]*_){%d}[^_]*)_(.*)' % (3), release_queue).groups()[0]

  print "I will analyze %s from %s to %s:" % (release_queue,start_tag,end_tag)

  # load the graph to check the history and identify merge prs and commits
  release_branch = RELEASES_BRANCHES[ release_queue ]
  graph = load_graph( release_branch , -1 )
 
  release_queue_results = {}
	
  release_queue_results[ 'release_name' ] = release_queue
  release_queue_results[ 'base_branch' ] = release_branch

  tags = execute_magic_command_tags( start_tag , end_tag , release_queue , release_branch )

  print 'I got these tags: '
  print tags

  release_queue_results['comparisons'] = compare_tags( tags , graph )

  find_static_results(release_queue_results['comparisons'])
  find_hlt_tests_results(release_queue_results['comparisons'])

  results.append(release_queue_results)
	
  print 
  print "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^"

cmsdist_tags_results = execute_magic_command_get_cmsdist_tags()

build_results = {}
execute_magic_command_find_results( build_results , 'builds' )

relvals_results = {}
execute_magic_command_find_results( relvals_results , 'relvals' )

unit_tests_results = {}
execute_magic_command_find_results( unit_tests_results , 'utests' )

addOn_tests_results = {}
execute_magic_command_find_results( addOn_tests_results ,'addOn' )


add_tests_to_results( results , unit_tests_results, relvals_results , addOn_tests_results, build_results , cmsdist_tags_results )
print_results(results)


structure = identify_release_groups(results)

generate_separated_json_results(results)

out_json = open("merged_prs_summary.json", "w")
json.dump(results,out_json,indent=4)
out_json.close()


out_groups = open("structure.json", "w")
json.dump(structure,out_groups,indent=4)
out_groups.close()


